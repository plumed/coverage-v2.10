<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">

<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>LCOV - plumed test coverage - pytorch/PytorchModel.cpp</title>
  <link rel="stylesheet" type="text/css" href="../gcov.css">
</head>

<body>

  <table width="100%" border=0 cellspacing=0 cellpadding=0>
    <tr><td class="title">LCOV - code coverage report</td></tr>
    <tr><td class="ruler"><img src="../glass.png" width=3 height=3 alt=""></td></tr>

    <tr>
      <td width="100%">
        <table cellpadding=1 border=0 width="100%">
          <tr>
            <td width="10%" class="headerItem">Current view:</td>
            <td width="35%" class="headerValue"><a href="../index.html">top level</a> - <a href="index.html">pytorch</a> - PytorchModel.cpp<span style="font-size: 80%;"> (source / <a href="PytorchModel.cpp.func-sort-c.html">functions</a>)</span></td>
            <td width="5%"></td>
            <td width="15%"></td>
            <td width="10%" class="headerCovTableHead">Hit</td>
            <td width="10%" class="headerCovTableHead">Total</td>
            <td width="15%" class="headerCovTableHead">Coverage</td>
          </tr>
          <tr>
            <td class="headerItem">Test:</td>
            <td class="headerValue">plumed test coverage</td>
            <td></td>
            <td class="headerItem">Lines:</td>
            <td class="headerCovTableEntry">62</td>
            <td class="headerCovTableEntry">69</td>
            <td class="headerCovTableEntryMed">89.9 %</td>
          </tr>
          <tr>
            <td class="headerItem">Date:</td>
            <td class="headerValue">2024-10-23 18:13:10</td>
            <td></td>
            <td class="headerItem">Functions:</td>
            <td class="headerCovTableEntry">4</td>
            <td class="headerCovTableEntry">5</td>
            <td class="headerCovTableEntryMed">80.0 %</td>
          </tr>
          <tr><td><img src="../glass.png" width=3 height=3 alt=""></td></tr>
        </table>
      </td>
    </tr>

    <tr><td class="ruler"><img src="../glass.png" width=3 height=3 alt=""></td></tr>
  </table>

  <table cellpadding=0 cellspacing=0 border=0>
    <tr>
      <td><br></td>
    </tr>
    <tr>
      <td>
<pre class="sourceHeading">          Line data    Source code</pre>
<pre class="source">
<a name="1"><span class="lineNum">       1 </span>            : /* +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++</a>
<a name="2"><span class="lineNum">       2 </span>            : Copyright (c) 2022-2023 of Luigi Bonati and Enrico Trizio.</a>
<a name="3"><span class="lineNum">       3 </span>            : </a>
<a name="4"><span class="lineNum">       4 </span>            : The pytorch module is free software: you can redistribute it and/or modify</a>
<a name="5"><span class="lineNum">       5 </span>            : it under the terms of the GNU Lesser General Public License as published by</a>
<a name="6"><span class="lineNum">       6 </span>            : the Free Software Foundation, either version 3 of the License, or</a>
<a name="7"><span class="lineNum">       7 </span>            : (at your option) any later version.</a>
<a name="8"><span class="lineNum">       8 </span>            : </a>
<a name="9"><span class="lineNum">       9 </span>            : The pytorch module is distributed in the hope that it will be useful,</a>
<a name="10"><span class="lineNum">      10 </span>            : but WITHOUT ANY WARRANTY; without even the implied warranty of</a>
<a name="11"><span class="lineNum">      11 </span>            : MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the</a>
<a name="12"><span class="lineNum">      12 </span>            : GNU Lesser General Public License for more details.</a>
<a name="13"><span class="lineNum">      13 </span>            : </a>
<a name="14"><span class="lineNum">      14 </span>            : You should have received a copy of the GNU Lesser General Public License</a>
<a name="15"><span class="lineNum">      15 </span>            : along with plumed.  If not, see &lt;http://www.gnu.org/licenses/&gt;.</a>
<a name="16"><span class="lineNum">      16 </span>            : +++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++ */</a>
<a name="17"><span class="lineNum">      17 </span>            : </a>
<a name="18"><span class="lineNum">      18 </span>            : #ifdef __PLUMED_HAS_LIBTORCH</a>
<a name="19"><span class="lineNum">      19 </span>            : // convert LibTorch version to string</a>
<a name="20"><span class="lineNum">      20 </span>            : //#define STRINGIFY(x) #x</a>
<a name="21"><span class="lineNum">      21 </span>            : //#define TOSTR(x) STRINGIFY(x)</a>
<a name="22"><span class="lineNum">      22 </span>            : //#define LIBTORCH_VERSION TO_STR(TORCH_VERSION_MAJOR) &quot;.&quot; TO_STR(TORCH_VERSION_MINOR) &quot;.&quot; TO_STR(TORCH_VERSION_PATCH)</a>
<a name="23"><span class="lineNum">      23 </span>            : </a>
<a name="24"><span class="lineNum">      24 </span>            : #include &quot;core/PlumedMain.h&quot;</a>
<a name="25"><span class="lineNum">      25 </span>            : #include &quot;function/Function.h&quot;</a>
<a name="26"><span class="lineNum">      26 </span>            : #include &quot;core/ActionRegister.h&quot;</a>
<a name="27"><span class="lineNum">      27 </span>            : </a>
<a name="28"><span class="lineNum">      28 </span>            : #include &lt;torch/torch.h&gt;</a>
<a name="29"><span class="lineNum">      29 </span>            : #include &lt;torch/script.h&gt;</a>
<a name="30"><span class="lineNum">      30 </span>            : </a>
<a name="31"><span class="lineNum">      31 </span>            : #include &lt;fstream&gt;</a>
<a name="32"><span class="lineNum">      32 </span>            : #include &lt;cmath&gt;</a>
<a name="33"><span class="lineNum">      33 </span>            : </a>
<a name="34"><span class="lineNum">      34 </span>            : // Note: Freezing a ScriptModule (torch::jit::freeze) works only in &gt;=1.11</a>
<a name="35"><span class="lineNum">      35 </span>            : // For 1.8 &lt;= versions &lt;=1.10 we need a hack</a>
<a name="36"><span class="lineNum">      36 </span>            : // (see https://discuss.pytorch.org/t/how-to-check-libtorch-version/77709/4 and also</a>
<a name="37"><span class="lineNum">      37 </span>            : // https://github.com/pytorch/pytorch/blob/dfbd030854359207cb3040b864614affeace11ce/torch/csrc/jit/api/module.cpp#L479)</a>
<a name="38"><span class="lineNum">      38 </span>            : // adapted from NequIP https://github.com/mir-group/nequip</a>
<a name="39"><span class="lineNum">      39 </span>            : #if ( TORCH_VERSION_MAJOR == 2 || TORCH_VERSION_MAJOR == 1 &amp;&amp; TORCH_VERSION_MINOR &lt;= 10 )</a>
<a name="40"><span class="lineNum">      40 </span>            : #define DO_TORCH_FREEZE_HACK</a>
<a name="41"><span class="lineNum">      41 </span>            : // For the hack, need more headers:</a>
<a name="42"><span class="lineNum">      42 </span>            : #include &lt;torch/csrc/jit/passes/freeze_module.h&gt;</a>
<a name="43"><span class="lineNum">      43 </span>            : #include &lt;torch/csrc/jit/passes/frozen_graph_optimizations.h&gt;</a>
<a name="44"><span class="lineNum">      44 </span>            : #endif</a>
<a name="45"><span class="lineNum">      45 </span>            : </a>
<a name="46"><span class="lineNum">      46 </span>            : using namespace std;</a>
<a name="47"><span class="lineNum">      47 </span>            : </a>
<a name="48"><span class="lineNum">      48 </span>            : namespace PLMD {</a>
<a name="49"><span class="lineNum">      49 </span>            : namespace function {</a>
<a name="50"><span class="lineNum">      50 </span>            : namespace pytorch {</a>
<a name="51"><span class="lineNum">      51 </span>            : </a>
<a name="52"><span class="lineNum">      52 </span>            : //+PLUMEDOC PYTORCH_FUNCTION PYTORCH_MODEL</a>
<a name="53"><span class="lineNum">      53 </span>            : /*</a>
<a name="54"><span class="lineNum">      54 </span>            : Load a PyTorch model compiled with TorchScript.</a>
<a name="55"><span class="lineNum">      55 </span>            : </a>
<a name="56"><span class="lineNum">      56 </span>            : This can be a function defined in Python or a more complex model, such as a neural network optimized on a set of data. In both cases the derivatives of the outputs with respect to the inputs are computed using the automatic differentiation (autograd) feature of Pytorch.</a>
<a name="57"><span class="lineNum">      57 </span>            : </a>
<a name="58"><span class="lineNum">      58 </span>            : By default it is assumed that the model is saved as: `model.ptc`, unless otherwise indicated by the `FILE` keyword. The function automatically checks for the number of output dimensions and creates a component for each of them. The outputs are called node-i with i between 0 and N-1 for N outputs.</a>
<a name="59"><span class="lineNum">      59 </span>            : </a>
<a name="60"><span class="lineNum">      60 </span>            : Note that this function requires \ref installation-libtorch LibTorch C++ library. Check the instructions in the \ref PYTORCH page to enable the module.</a>
<a name="61"><span class="lineNum">      61 </span>            : </a>
<a name="62"><span class="lineNum">      62 </span>            : \par Examples</a>
<a name="63"><span class="lineNum">      63 </span>            : Load a model called `torch_model.ptc` that takes as input two dihedral angles and returns two outputs.</a>
<a name="64"><span class="lineNum">      64 </span>            : </a>
<a name="65"><span class="lineNum">      65 </span>            : \plumedfile</a>
<a name="66"><span class="lineNum">      66 </span>            : #SETTINGS AUXFILE=regtest/pytorch/rt-pytorch_model_2d/torch_model.ptc</a>
<a name="67"><span class="lineNum">      67 </span>            : phi: TORSION ATOMS=5,7,9,15</a>
<a name="68"><span class="lineNum">      68 </span>            : psi: TORSION ATOMS=7,9,15,17</a>
<a name="69"><span class="lineNum">      69 </span>            : model: PYTORCH_MODEL FILE=torch_model.ptc ARG=phi,psi</a>
<a name="70"><span class="lineNum">      70 </span>            : PRINT FILE=COLVAR ARG=model.node-0,model.node-1</a>
<a name="71"><span class="lineNum">      71 </span>            : \endplumedfile</a>
<a name="72"><span class="lineNum">      72 </span>            : </a>
<a name="73"><span class="lineNum">      73 </span>            : */</a>
<a name="74"><span class="lineNum">      74 </span>            : //+ENDPLUMEDOC</a>
<a name="75"><span class="lineNum">      75 </span>            : </a>
<a name="76"><span class="lineNum">      76 </span>            : </a>
<a name="77"><span class="lineNum">      77 </span>            : class PytorchModel :</a>
<a name="78"><span class="lineNum">      78 </span>            :   public Function</a>
<a name="79"><span class="lineNum">      79 </span>            : {</a>
<a name="80"><span class="lineNum">      80 </span>            :   unsigned _n_in;</a>
<a name="81"><span class="lineNum">      81 </span>            :   unsigned _n_out;</a>
<a name="82"><span class="lineNum">      82 </span>            :   torch::jit::script::Module _model;</a>
<a name="83"><span class="lineNum">      83 </span>            :   torch::Device device = torch::kCPU;</a>
<a name="84"><span class="lineNum">      84 </span>            : </a>
<a name="85"><span class="lineNum">      85 </span>            : public:</a>
<a name="86"><span class="lineNum">      86 </span>            :   explicit PytorchModel(const ActionOptions&amp;);</a>
<a name="87"><span class="lineNum">      87 </span>            :   void calculate();</a>
<a name="88"><span class="lineNum">      88 </span>            :   static void registerKeywords(Keywords&amp; keys);</a>
<a name="89"><span class="lineNum">      89 </span>            : </a>
<a name="90"><span class="lineNum">      90 </span>            :   std::vector&lt;float&gt; tensor_to_vector(const torch::Tensor&amp; x);</a>
<a name="91"><span class="lineNum">      91 </span>            : };</a>
<a name="92"><span class="lineNum">      92 </span>            : </a>
<a name="93"><span class="lineNum">      93 </span>            : PLUMED_REGISTER_ACTION(PytorchModel,&quot;PYTORCH_MODEL&quot;)</a>
<a name="94"><span class="lineNum">      94 </span>            : </a>
<a name="95"><span class="lineNum">      95 </span><span class="lineCov">          6 : void PytorchModel::registerKeywords(Keywords&amp; keys) {</span></a>
<a name="96"><span class="lineNum">      96 </span><span class="lineCov">          6 :   Function::registerKeywords(keys);</span></a>
<a name="97"><span class="lineNum">      97 </span><span class="lineCov">          6 :   keys.use(&quot;ARG&quot;);</span></a>
<a name="98"><span class="lineNum">      98 </span><span class="lineCov">         12 :   keys.add(&quot;optional&quot;,&quot;FILE&quot;,&quot;Filename of the PyTorch compiled model&quot;);</span></a>
<a name="99"><span class="lineNum">      99 </span><span class="lineCov">         12 :   keys.addOutputComponent(&quot;node&quot;, &quot;default&quot;, &quot;Model outputs&quot;);</span></a>
<a name="100"><span class="lineNum">     100 </span><span class="lineCov">          6 : }</span></a>
<a name="101"><span class="lineNum">     101 </span>            : </a>
<a name="102"><span class="lineNum">     102 </span>            : // Auxiliary function to transform torch tensors in std vectors</a>
<a name="103"><span class="lineNum">     103 </span><span class="lineCov">        103 : std::vector&lt;float&gt; PytorchModel::tensor_to_vector(const torch::Tensor&amp; x) {</span></a>
<a name="104"><span class="lineNum">     104 </span><span class="lineCov">        206 :   return std::vector&lt;float&gt;(x.data_ptr&lt;float&gt;(), x.data_ptr&lt;float&gt;() + x.numel());</span></a>
<a name="105"><span class="lineNum">     105 </span>            : }</a>
<a name="106"><span class="lineNum">     106 </span>            : </a>
<a name="107"><span class="lineNum">     107 </span><span class="lineCov">          4 : PytorchModel::PytorchModel(const ActionOptions&amp;ao):</span></a>
<a name="108"><span class="lineNum">     108 </span>            :   Action(ao),</a>
<a name="109"><span class="lineNum">     109 </span><span class="lineCov">          4 :   Function(ao)</span></a>
<a name="110"><span class="lineNum">     110 </span>            : {</a>
<a name="111"><span class="lineNum">     111 </span>            :   // print libtorch version</a>
<a name="112"><span class="lineNum">     112 </span><span class="lineCov">          4 :   std::stringstream ss;</span></a>
<a name="113"><span class="lineNum">     113 </span><span class="lineCov">          4 :   ss &lt;&lt; TORCH_VERSION_MAJOR &lt;&lt; &quot;.&quot; &lt;&lt; TORCH_VERSION_MINOR &lt;&lt; &quot;.&quot; &lt;&lt; TORCH_VERSION_PATCH;</span></a>
<a name="114"><span class="lineNum">     114 </span>            :   std::string version;</a>
<a name="115"><span class="lineNum">     115 </span><span class="lineCov">          4 :   ss &gt;&gt; version; // extract into the string.</span></a>
<a name="116"><span class="lineNum">     116 </span><span class="lineCov">          8 :   log.printf((&quot;  LibTorch version: &quot;+version+&quot;\n&quot;).data());</span></a>
<a name="117"><span class="lineNum">     117 </span>            : </a>
<a name="118"><span class="lineNum">     118 </span>            :   //number of inputs of the model</a>
<a name="119"><span class="lineNum">     119 </span><span class="lineCov">          4 :   _n_in=getNumberOfArguments();</span></a>
<a name="120"><span class="lineNum">     120 </span>            : </a>
<a name="121"><span class="lineNum">     121 </span>            :   //parse model name</a>
<a name="122"><span class="lineNum">     122 </span><span class="lineCov">          4 :   std::string fname=&quot;model.ptc&quot;;</span></a>
<a name="123"><span class="lineNum">     123 </span><span class="lineCov">          8 :   parse(&quot;FILE&quot;,fname);</span></a>
<a name="124"><span class="lineNum">     124 </span>            : </a>
<a name="125"><span class="lineNum">     125 </span>            :   //deserialize the model from file</a>
<a name="126"><span class="lineNum">     126 </span>            :   try {</a>
<a name="127"><span class="lineNum">     127 </span><span class="lineCov">          4 :     _model = torch::jit::load(fname, device);</span></a>
<a name="128"><span class="lineNum">     128 </span>            :   }</a>
<a name="129"><span class="lineNum">     129 </span>            : </a>
<a name="130"><span class="lineNum">     130 </span>            :   //if an error is thrown check if the file exists or not</a>
<a name="131"><span class="lineNum">     131 </span><span class="lineNoCov">          0 :   catch (const c10::Error&amp; e) {</span></a>
<a name="132"><span class="lineNum">     132 </span><span class="lineNoCov">          0 :     std::ifstream infile(fname);</span></a>
<a name="133"><span class="lineNum">     133 </span>            :     bool exist = infile.good();</a>
<a name="134"><span class="lineNum">     134 </span><span class="lineNoCov">          0 :     infile.close();</span></a>
<a name="135"><span class="lineNum">     135 </span><span class="lineNoCov">          0 :     if (exist) {</span></a>
<a name="136"><span class="lineNum">     136 </span><span class="lineNoCov">          0 :       plumed_merror(&quot;Cannot load FILE: '&quot;+fname+&quot;'. Please check that it is a Pytorch compiled model (exported with 'torch.jit.trace' or 'torch.jit.script').&quot;);</span></a>
<a name="137"><span class="lineNum">     137 </span>            :     }</a>
<a name="138"><span class="lineNum">     138 </span>            :     else {</a>
<a name="139"><span class="lineNum">     139 </span><span class="lineNoCov">          0 :       plumed_merror(&quot;The FILE: '&quot;+fname+&quot;' does not exist.&quot;);</span></a>
<a name="140"><span class="lineNum">     140 </span>            :     }</a>
<a name="141"><span class="lineNum">     141 </span><span class="lineNoCov">          0 :   }</span></a>
<a name="142"><span class="lineNum">     142 </span><span class="lineCov">          4 :   checkRead();</span></a>
<a name="143"><span class="lineNum">     143 </span>            : </a>
<a name="144"><span class="lineNum">     144 </span>            : // Optimize model</a>
<a name="145"><span class="lineNum">     145 </span>            :   _model.eval();</a>
<a name="146"><span class="lineNum">     146 </span>            : #ifdef DO_TORCH_FREEZE_HACK</a>
<a name="147"><span class="lineNum">     147 </span>            :   // Do the hack</a>
<a name="148"><span class="lineNum">     148 </span>            :   // Copied from the implementation of torch::jit::freeze,</a>
<a name="149"><span class="lineNum">     149 </span>            :   // except without the broken check</a>
<a name="150"><span class="lineNum">     150 </span>            :   // See https://github.com/pytorch/pytorch/blob/dfbd030854359207cb3040b864614affeace11ce/torch/csrc/jit/api/module.cpp</a>
<a name="151"><span class="lineNum">     151 </span>            :   bool optimize_numerics = true;  // the default</a>
<a name="152"><span class="lineNum">     152 </span>            :   // the {} is preserved_attrs</a>
<a name="153"><span class="lineNum">     153 </span>            :   auto out_mod = torch::jit::freeze_module(</a>
<a name="154"><span class="lineNum">     154 </span>            :                    _model, {}</a>
<a name="155"><span class="lineNum">     155 </span><span class="lineCov">          4 :                  );</span></a>
<a name="156"><span class="lineNum">     156 </span>            :   // See 1.11 bugfix in https://github.com/pytorch/pytorch/pull/71436</a>
<a name="157"><span class="lineNum">     157 </span><span class="lineCov">          8 :   auto graph = out_mod.get_method(&quot;forward&quot;).graph();</span></a>
<a name="158"><span class="lineNum">     158 </span><span class="lineCov">          4 :   OptimizeFrozenGraph(graph, optimize_numerics);</span></a>
<a name="159"><span class="lineNum">     159 </span><span class="lineCov">          4 :   _model = out_mod;</span></a>
<a name="160"><span class="lineNum">     160 </span>            : #else</a>
<a name="161"><span class="lineNum">     161 </span>            :   // Do it normally</a>
<a name="162"><span class="lineNum">     162 </span>            :   _model = torch::jit::freeze(_model);</a>
<a name="163"><span class="lineNum">     163 </span>            : #endif</a>
<a name="164"><span class="lineNum">     164 </span>            : </a>
<a name="165"><span class="lineNum">     165 </span>            : // Optimize model for inference</a>
<a name="166"><span class="lineNum">     166 </span>            : #if (TORCH_VERSION_MAJOR == 2 || TORCH_VERSION_MAJOR == 1 &amp;&amp; TORCH_VERSION_MINOR &gt;= 10)</a>
<a name="167"><span class="lineNum">     167 </span><span class="lineCov">          4 :   _model = torch::jit::optimize_for_inference(_model);</span></a>
<a name="168"><span class="lineNum">     168 </span>            : #endif</a>
<a name="169"><span class="lineNum">     169 </span>            : </a>
<a name="170"><span class="lineNum">     170 </span>            :   //check the dimension of the output</a>
<a name="171"><span class="lineNum">     171 </span><span class="lineCov">          4 :   log.printf(&quot;  Checking output dimension:\n&quot;);</span></a>
<a name="172"><span class="lineNum">     172 </span><span class="lineCov">          4 :   std::vector&lt;float&gt; input_test (_n_in);</span></a>
<a name="173"><span class="lineNum">     173 </span><span class="lineCov">          4 :   torch::Tensor single_input = torch::tensor(input_test).view({1,_n_in});</span></a>
<a name="174"><span class="lineNum">     174 </span><span class="lineCov">          8 :   single_input = single_input.to(device);</span></a>
<a name="175"><span class="lineNum">     175 </span>            :   std::vector&lt;torch::jit::IValue&gt; inputs;</a>
<a name="176"><span class="lineNum">     176 </span><span class="lineCov">          4 :   inputs.push_back( single_input );</span></a>
<a name="177"><span class="lineNum">     177 </span><span class="lineCov">          8 :   torch::Tensor output = _model.forward( inputs ).toTensor();</span></a>
<a name="178"><span class="lineNum">     178 </span><span class="lineCov">          4 :   vector&lt;float&gt; cvs = this-&gt;tensor_to_vector (output);</span></a>
<a name="179"><span class="lineNum">     179 </span><span class="lineCov">          4 :   _n_out=cvs.size();</span></a>
<a name="180"><span class="lineNum">     180 </span>            : </a>
<a name="181"><span class="lineNum">     181 </span>            :   //create components</a>
<a name="182"><span class="lineNum">     182 </span><span class="lineCov">          9 :   for(unsigned j=0; j&lt;_n_out; j++) {</span></a>
<a name="183"><span class="lineNum">     183 </span><span class="lineCov">          5 :     string name_comp = &quot;node-&quot;+std::to_string(j);</span></a>
<a name="184"><span class="lineNum">     184 </span><span class="lineCov">          5 :     addComponentWithDerivatives( name_comp );</span></a>
<a name="185"><span class="lineNum">     185 </span><span class="lineCov">          5 :     componentIsNotPeriodic( name_comp );</span></a>
<a name="186"><span class="lineNum">     186 </span>            :   }</a>
<a name="187"><span class="lineNum">     187 </span>            : </a>
<a name="188"><span class="lineNum">     188 </span>            :   //print log</a>
<a name="189"><span class="lineNum">     189 </span><span class="lineCov">          4 :   log.printf(&quot;  Number of input: %d \n&quot;,_n_in);</span></a>
<a name="190"><span class="lineNum">     190 </span><span class="lineCov">          4 :   log.printf(&quot;  Number of outputs: %d \n&quot;,_n_out);</span></a>
<a name="191"><span class="lineNum">     191 </span><span class="lineCov">          4 :   log.printf(&quot;  Bibliography: &quot;);</span></a>
<a name="192"><span class="lineNum">     192 </span><span class="lineCov">          8 :   log&lt;&lt;plumed.cite(&quot;Bonati, Trizio, Rizzi and Parrinello, J. Chem. Phys. 159, 014801 (2023)&quot;);</span></a>
<a name="193"><span class="lineNum">     193 </span><span class="lineCov">          8 :   log&lt;&lt;plumed.cite(&quot;Bonati, Rizzi and Parrinello, J. Phys. Chem. Lett. 11, 2998-3004 (2020)&quot;);</span></a>
<a name="194"><span class="lineNum">     194 </span><span class="lineCov">          4 :   log.printf(&quot;\n&quot;);</span></a>
<a name="195"><span class="lineNum">     195 </span>            : </a>
<a name="196"><span class="lineNum">     196 </span><span class="lineCov">         12 : }</span></a>
<a name="197"><span class="lineNum">     197 </span>            : </a>
<a name="198"><span class="lineNum">     198 </span>            : </a>
<a name="199"><span class="lineNum">     199 </span><span class="lineCov">         44 : void PytorchModel::calculate() {</span></a>
<a name="200"><span class="lineNum">     200 </span>            : </a>
<a name="201"><span class="lineNum">     201 </span>            :   // retrieve arguments</a>
<a name="202"><span class="lineNum">     202 </span><span class="lineCov">         44 :   vector&lt;float&gt; current_S(_n_in);</span></a>
<a name="203"><span class="lineNum">     203 </span><span class="lineCov">         99 :   for(unsigned i=0; i&lt;_n_in; i++)</span></a>
<a name="204"><span class="lineNum">     204 </span><span class="lineCov">         55 :     current_S[i]=getArgument(i);</span></a>
<a name="205"><span class="lineNum">     205 </span>            :   //convert to tensor</a>
<a name="206"><span class="lineNum">     206 </span><span class="lineCov">         44 :   torch::Tensor input_S = torch::tensor(current_S).view({1,_n_in}).to(device);</span></a>
<a name="207"><span class="lineNum">     207 </span>            :   input_S.set_requires_grad(true);</a>
<a name="208"><span class="lineNum">     208 </span>            :   //convert to Ivalue</a>
<a name="209"><span class="lineNum">     209 </span>            :   std::vector&lt;torch::jit::IValue&gt; inputs;</a>
<a name="210"><span class="lineNum">     210 </span><span class="lineCov">         44 :   inputs.push_back( input_S );</span></a>
<a name="211"><span class="lineNum">     211 </span>            :   //calculate output</a>
<a name="212"><span class="lineNum">     212 </span><span class="lineCov">         88 :   torch::Tensor output = _model.forward( inputs ).toTensor();</span></a>
<a name="213"><span class="lineNum">     213 </span>            : </a>
<a name="214"><span class="lineNum">     214 </span>            : </a>
<a name="215"><span class="lineNum">     215 </span><span class="lineCov">         99 :   for(unsigned j=0; j&lt;_n_out; j++) {</span></a>
<a name="216"><span class="lineNum">     216 </span><span class="lineCov">         55 :     auto grad_output = torch::ones({1}).expand({1, 1}).to(device);</span></a>
<a name="217"><span class="lineNum">     217 </span><span class="lineCov">        440 :     auto gradient = torch::autograd::grad({output.slice(/*dim=*/1, /*start=*/j, /*end=*/j+1)},</span></a>
<a name="218"><span class="lineNum">     218 </span>            :     {input_S},</a>
<a name="219"><span class="lineNum">     219 </span>            :     /*grad_outputs=*/ {grad_output},</a>
<a name="220"><span class="lineNum">     220 </span>            :     /*retain_graph=*/true,</a>
<a name="221"><span class="lineNum">     221 </span>            :     /*create_graph=*/false)[0]; // the [0] is to get a tensor and not a vector&lt;at::tensor&gt;</a>
<a name="222"><span class="lineNum">     222 </span>            : </a>
<a name="223"><span class="lineNum">     223 </span><span class="lineCov">         55 :     vector&lt;float&gt; der = this-&gt;tensor_to_vector ( gradient );</span></a>
<a name="224"><span class="lineNum">     224 </span><span class="lineCov">         55 :     string name_comp = &quot;node-&quot;+std::to_string(j);</span></a>
<a name="225"><span class="lineNum">     225 </span>            :     //set derivatives of component j</a>
<a name="226"><span class="lineNum">     226 </span><span class="lineCov">        132 :     for(unsigned i=0; i&lt;_n_in; i++)</span></a>
<a name="227"><span class="lineNum">     227 </span><span class="lineCov">         77 :       setDerivative( getPntrToComponent(name_comp),i, der[i] );</span></a>
<a name="228"><span class="lineNum">     228 </span>            :   }</a>
<a name="229"><span class="lineNum">     229 </span>            : </a>
<a name="230"><span class="lineNum">     230 </span>            :   //set CV values</a>
<a name="231"><span class="lineNum">     231 </span><span class="lineCov">         44 :   vector&lt;float&gt; cvs = this-&gt;tensor_to_vector (output);</span></a>
<a name="232"><span class="lineNum">     232 </span><span class="lineCov">         99 :   for(unsigned j=0; j&lt;_n_out; j++) {</span></a>
<a name="233"><span class="lineNum">     233 </span><span class="lineCov">         55 :     string name_comp = &quot;node-&quot;+std::to_string(j);</span></a>
<a name="234"><span class="lineNum">     234 </span><span class="lineCov">         55 :     getPntrToComponent(name_comp)-&gt;set(cvs[j]);</span></a>
<a name="235"><span class="lineNum">     235 </span>            :   }</a>
<a name="236"><span class="lineNum">     236 </span>            : </a>
<a name="237"><span class="lineNum">     237 </span><span class="lineCov">         88 : }</span></a>
<a name="238"><span class="lineNum">     238 </span>            : </a>
<a name="239"><span class="lineNum">     239 </span>            : </a>
<a name="240"><span class="lineNum">     240 </span>            : } //PLMD</a>
<a name="241"><span class="lineNum">     241 </span>            : } //function</a>
<a name="242"><span class="lineNum">     242 </span>            : } //pytorch</a>
<a name="243"><span class="lineNum">     243 </span>            : </a>
<a name="244"><span class="lineNum">     244 </span>            : #endif //PLUMED_HAS_LIBTORCH</a>
</pre>
      </td>
    </tr>
  </table>
  <br>

  <table width="100%" border=0 cellspacing=0 cellpadding=0>
    <tr><td class="ruler"><img src="../glass.png" width=3 height=3 alt=""></td></tr>
    <tr><td class="versionInfo">Generated by: <a href="https://github.com/linux-test-project/lcov" target="_parent">LCOV version 1.16</a></td></tr>
  </table>
  <br>

</body>
</html>
